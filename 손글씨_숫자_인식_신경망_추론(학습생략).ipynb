{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "손글씨 숫자 인식_신경망_추론(학습생략).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1gPEc9dRGnD1cTY34Wcfgrm-oECt9ynBY",
      "authorship_tag": "ABX9TyMKuGEEFfVI8f1tFavnzCOa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeaeunJi/deep_learning-/blob/main/%EC%86%90%EA%B8%80%EC%94%A8_%EC%88%AB%EC%9E%90_%EC%9D%B8%EC%8B%9D_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EC%B6%94%EB%A1%A0(%ED%95%99%EC%8A%B5%EC%83%9D%EB%9E%B5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7itpVBif4VX"
      },
      "source": [
        "- 이미 학습된 매개변수를 사용하여 학습과정 생략 후 추론 과정만 구현함\n",
        "- 이러한 추론 과정을 신경망의 순전파(forward propagation)이라고 함\n",
        "- 신경망에서도 기계 학습과 마찬가지로 2단계로 문제를 해결함\n",
        "  - 학습 : 학습 데이터를 사용해 가중치 매개변수 학습\n",
        "  - 추론 : 학습한 매개변수를 사용하여 입력 데이터 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfRzC2mRgSKW"
      },
      "source": [
        "### MNIST 데이터셋\n",
        "- 기계 학습 분야에서 아주 유명한 데이터셋\n",
        "- 0~9 까지의 숫자 이미지로 구성되며, 훈련 이미지 60,000장, 시험 이미지 10,000장\n",
        "- MNIST의 이미지 데이넌 28*28 크기의 회색조 이미지(1채널)\n",
        "- 각 픽셀은 0에서 255까지의 값을 가짐\n",
        "- 각 이미지별 실제 의미하는 숫자가 레이블로 존재"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kJlVD6hmblQ"
      },
      "source": [
        "# coding: utf-8\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('You should use Python 3.x')\n",
        "import os.path\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "__file__ = os.pardir\n",
        "# print(__file__)\n",
        "dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print(\"Downloading \" + file_name + \" ... \")\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    print(\"Done\")\n",
        "    \n",
        "def download_mnist():\n",
        "    for v in key_file.values():\n",
        "       _download(v)\n",
        "        \n",
        "def _load_label(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, img_size)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return data\n",
        "    \n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    download_mnist()\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "        \n",
        "    return T\n",
        "    \n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"MNIST 데이터셋 읽기\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
        "    one_hot_label : \n",
        "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
        "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
        "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "        \n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    \n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "            \n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    \n",
        "    \n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     init_mnist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOvelZAjgL5i",
        "outputId": "debe2299-5976-491c-e7cb-aeb83ca10bf3"
      },
      "source": [
        "# MNIST 데이터셋을 내려받아 그 이미지를 넘파이 배열로 변화해주는 파이썬 스크립트 사용\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "# from dataset.mnist import load_mnist\n",
        "# load_mnist()\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "  load_mnist(flatten=True, normalize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXH3NIUGhpUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e032614-f6e2-4759-cc17-3e1a352aff51"
      },
      "source": [
        "# MNIST 이미지를 화면에 표출\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def img_show(img):\n",
        "  pil_img = Image.fromarray(np.uint8(img))\n",
        "  pil_img.show()\n",
        "\n",
        "img = x_train[0]\n",
        "label = t_train[0]\n",
        "print(label)\n",
        "\n",
        "img = img.reshape(28,28)\n",
        "print(img.shape)\n",
        "img_show(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uzlye8qoE4h"
      },
      "source": [
        "import pickle \n",
        "\n",
        "def get_data() :\n",
        "  (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False, one_hot_label=False)\n",
        "  return x_test, t_test\n",
        "\n",
        "def init_network():\n",
        "  with open('sample_weight.pkl', 'rb') as f:\n",
        "    network = pickle.load(f)\n",
        "  return network\n",
        "\n",
        "def sigmoid(x) :\n",
        "  return 1 / (1+np.exp(-x)  )\n",
        "\n",
        "def softmax(x):\n",
        "  c = np.max(x)\n",
        "  exp_a = np.exp(x-c)\n",
        "  sum_exp_a = np.sum(exp_a)\n",
        "  y = exp_a / sum_exp_a\n",
        "\n",
        "  return y\n",
        "\n",
        "def predict(network, x) :\n",
        "  W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
        "  b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
        "\n",
        "  a1 = np.dot(x, W1) + b1\n",
        "  z1 = sigmoid(a1)\n",
        "  a2 = np.dot(z1, W2) + b2\n",
        "  z2 = sigmoid(a2)\n",
        "  a3 = np.dot(z2, W3) + b3\n",
        "  y = softmax(a3)\n",
        "\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm6pwe92qvul"
      },
      "source": [
        "# # sample_weight.pkl 다운로드 후 파일 업로드\n",
        "# url = \"https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch03/sample_weight.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn2KFGMQrMQQ",
        "outputId": "9fbdb87e-6c3b-41d8-f221-d43d19315e77"
      },
      "source": [
        "x, t = get_data()\n",
        "network = init_network()\n",
        "\n",
        "accuracy_cnt = 0\n",
        "for i in range(len(x)) :\n",
        "  y = predict(network, x[i])\n",
        "  p = np.argmax(y) # 확률이 가장 높은 원소의 인덱스\n",
        "  if p == t[i] :\n",
        "    accuracy_cnt += 1\n",
        "\n",
        "print('Accuracy : ', str(float(accuracy_cnt / len(x))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..:13: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  0.9207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci9cuuNtOcg_"
      },
      "source": [
        "### 배치 처리\n",
        "- MNIST와 같은 이미지 데이터는 하나 당 784개의 원소를 가지고 있음\n",
        "- 이러한 이미지를 한번에 여러 장 입력하는 경우에는 입력 데이터의 형상이 100*784와 같을 것임 \n",
        "\n",
        "    ==> 이렇게 하나에 데이터로 묶은 입력 데이터를 '배치(batch)'라고 함\n",
        "\n",
        "- 배치 처리의 장점 : 이미지 1장당 처리 시간을 줄여줌\n",
        "  - 1) 수치 계산 라이브러리들은 대부분 크기가 큰 배열 처리에 대한 고도의 최적화가 이루어져 있음\n",
        "  - 2) 데이터를 읽고 쓰는 느린 I/O속도보다 빠른 CPU나 GPU로 순수 계산을 수행함으로써, 분할된 작은 배열을 여러 번 계산하는 것보다 큰 배열을 한꺼번에 계산하는 것이 빠름"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5f6gNegrm5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082433b4-bc32-40db-a4f0-1ea20a69dde5"
      },
      "source": [
        "# 배치 처리 구현해보기\n",
        "x, t = get_data()\n",
        "network = init_network()\n",
        "\n",
        "batch_size = 100 # 배치 크기\n",
        "accuracy_cnt = 0\n",
        "\n",
        "for i in range(0, len(x), batch_size) :\n",
        "  x_batch = x[i:i+batch_size]\n",
        "  y_batch = predict(network, x_batch)\n",
        "  p = np.argmax(y_batch, axis=1) # 첫번째 차원(100*10의 배열 중 첫번째 차원을 구성하는 각 원소에서 최댓값 인덱스를 찾도록, 차원도 0부터 시작)\n",
        "  accuracy_cnt += np.sum( p == t[i:i+batch_size])\n",
        "\n",
        "print('Accuracy : ', str(float(accuracy_cnt / len(x))))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  0.9207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "..:13: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S-M78lROgVH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}