{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어와 단어의 분산표현.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhV094soN449J1Da935fW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeaeunJi/deep_learning-/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%99%80_%EB%8B%A8%EC%96%B4%EC%9D%98_%EB%B6%84%EC%82%B0%ED%91%9C%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1mewwV66OgN"
      },
      "source": [
        "### 파이썬으로 말뭉치 처리(corpus)\r\n",
        "- 간단한 전처리(preprocessing) ==> 텍스트 데이터를 단어로 분할하고, 분할된 단어들을 단어 id 리스트로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHtvw-E6iXT",
        "outputId": "60d02cb6-53de-4e73-fac4-5dd77c609153"
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\r\n",
        "text = text.lower() # 모든 문자를 소문화 변환\r\n",
        "text = text.replace('.', ' .') # 문장의 단위를 끊는 '.'를 공백이 포함된 ' .'로 대체\r\n",
        "\r\n",
        "print(text)\r\n",
        "\r\n",
        "# 단어 추출\r\n",
        "words = text.split(' ') #\r\n",
        "print(words)\r\n",
        "\r\n",
        "\r\n",
        "# 단어 분할을 위해 마침표 앞에 공백을 넣는 것 이외에 정규표현식(regular expression)을 이용하면 더 범용적임\r\n",
        "# import re\r\n",
        "# words = re.split('(\\W+)?',text)\r\n",
        "# print(words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you say goodbye and i say hello .\n",
            "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtpaMr0k6yJt",
        "outputId": "8c4fd0ce-dd78-466b-f5f8-2cb0b9001252"
      },
      "source": [
        "# 단어에 id를 부여하여, 단어와 단어 id별 대응되도록 파이썬의 dict 객체 생성\r\n",
        "word_to_id = {}\r\n",
        "id_to_word= {}\r\n",
        "\r\n",
        "for word in words :\r\n",
        "  if word not in word_to_id : # 단어 리스트에 없는 단어일 경우 추가\r\n",
        "    new_id = len(word_to_id)\r\n",
        "    word_to_id[word] = new_id\r\n",
        "    id_to_word[new_id] = word\r\n",
        "\r\n",
        "print('key : id, value : word인 dict', id_to_word)\r\n",
        "print('key : word, value : id인 dict', word_to_id)\r\n",
        "\r\n",
        "# 단어로 단어 id를 검색하거나 반대의 경우도 검색 가능해짐"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "key : id, value : word인 dict {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
            "key : word, value : id인 dict {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjeVadBk8Png",
        "outputId": "349d5d09-48ab-4c68-95d8-115ecb6b8e3f"
      },
      "source": [
        "# 단어 목록을 단어 id 목록으로 변경\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "corpus = [ word_to_id[w] for w in words]\r\n",
        "corpus = np.array(corpus)\r\n",
        "print(corpus)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2u2fdbJ8-n5"
      },
      "source": [
        "# 구현한 함수를 모아서 preprocess() 함수로 구현\r\n",
        "def preprocess(text) :\r\n",
        "  # 입력받은 text 데이터에서 공백을 기준으로 단어 추출\r\n",
        "  text = text.lower()\r\n",
        "  text = text.replace('.', ' .')\r\n",
        "  words = text.split(' ')\r\n",
        "  \r\n",
        "  word_to_id = {}\r\n",
        "  id_to_word = {}\r\n",
        "\r\n",
        "  for word in words : \r\n",
        "    if word not in word_to_id :\r\n",
        "      new_id = len(word_to_id)\r\n",
        "      word_to_id[word] = new_id\r\n",
        "      id_to_word[new_id] = word\r\n",
        "    \r\n",
        "  corpus = np.array([word_to_id[w] for w in words])\r\n",
        "\r\n",
        "  return corpus, word_to_id, id_to_word\r\n",
        "\r\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i2if0Tv91Q_",
        "outputId": "c6c8e132-e798-465a-bd7b-6830f8e34feb"
      },
      "source": [
        "corpus, word_to_id, id_to_word = preprocess(text)\r\n",
        "print(corpus, word_to_id, id_to_word)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6 7] {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '': 6, '.': 7} {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '', 7: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_39PU6xh-Czw"
      },
      "source": [
        "### 단어의 분산 표현\r\n",
        "- 단어의 분산 표현('distributional representation) \r\n",
        "  - 텍스트인 단어를 '단어의 의미'를 정확하게 표현할 수 있는 벡터로 표현\r\n",
        "  - 단어를 고정 길이의 밀집 벡터로 표현(대부분이 0이 아닌 실수인 벡터)\r\n",
        "\r\n",
        "### 분포 가설\r\n",
        "- '단어의 의미는 주변 단어에 의해 형성된다'라는 가설\r\n",
        "\r\n",
        "- 자연어 처리에서 단어를 벡터로 표현하는 연구들은 '분포 가설(distributional hypothesis)'에서 착안됨\r\n",
        "\r\n",
        "- 단어 자체에는 의미가 없고, 그 단어가 사용된 맥락(context)가 의미를 형성한다고 봄\r\n",
        "  - 의미가 같은 단어들은 같은 맥락에서 더 자주 등장하며, 같은 맥락에서 사용되면 가까운 의미의 단어라는 것을 알 수 있음\r\n",
        "\r\n",
        "    예) I drink beer //  I drink wine // We guzzle beer // We guzzle wine\r\n",
        "    \r\n",
        "    drink : 마시다, guzzle : 폭음하다\r\n",
        "\r\n",
        "  - 맥락 : 주목하려는 단어의 주변에 높인 단어를 의미\r\n",
        "  - 윈도우 크기(window size) : 주변 단어를 몇개나 포함할지를 의미\r\n",
        "    \r\n",
        "    예) 윈도우 크기 = 2이면, 좌우 두 단어씩이 맥락에 포함됨(총 4단어)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXvW5nzRAjIk"
      },
      "source": [
        "\r\n",
        "### 동시발생 행렬(co-occurrence matrix)\r\n",
        "- 분포 가설을 기반으로 단어를 벡터로 나타내는 방법으로 '통계기반(statistical based) 기법'이 있음\r\n",
        "  - 주목한 단어의 주변에 어떤 단어가 몇 번 등장하는지(빈도)를 집계하는 방법\r\n",
        "  - 모든 단어 각각의 맥락에 해당하는 단어의 빈도\r\n",
        "\r\n",
        "  - 이를 활용하여 단어를 벡터로 표현 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC6WCpRO99Vn"
      },
      "source": [
        "# corpus에서 동시발생 행렬을 생성해주는 함수\r\n",
        "def create_co_matrix(corpus, vocab_size, window_size=1) :\r\n",
        "  corpus_size = len(corpus)\r\n",
        "  co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32) # 0으로 채워진 2차원 배열\r\n",
        "\r\n",
        "  for idx, word_id in enumerate(corpus) :\r\n",
        "    for i in range(1, window_size+1) :\r\n",
        "      left_idx = idx - i\r\n",
        "      right_idx = idx + i\r\n",
        "\r\n",
        "      if left_idx >= 0 :\r\n",
        "        left_word_id = corpus[left_idx]\r\n",
        "        co_matrix[word_id, left_word_id] += 1\r\n",
        "\r\n",
        "      if right_idx < corpus_size : \r\n",
        "        right_word_id = corpus[right_idx]\r\n",
        "        co_matrix[word_id, right_word_id] += 1\r\n",
        "\r\n",
        "  return co_matrix"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WZlTiJXCq16"
      },
      "source": [
        "### 벡터 간 유사도\r\n",
        "- 동시발생 행렬을 활용하여 단어를 벡터로 표현한 후 이 벡터 사이의 유사도를 측정하는 방법이 필요\r\n",
        "\r\n",
        "- 벡터 간 유사도를 측정하는 방법에는 대표적으로 벡터의 내적이나 유클리드 거리 등 있지만, 단어 벡터의 유사도를 나타낼 때에는 '코사인 유사도(cosine similarity)'를 자주 사용\r\n",
        "\r\n",
        "  - 노름(norm) : 벡터의 크기\r\n",
        "  - 코사인 유사도에서는  L2 노름으로 계산(벡터의 각 원소를 제곱하여 더한 후의 제곱근)\r\n",
        "  - 벡터를 정규화하고 내적을 구하는 것이 코사인 유사도의 핵심(?)\r\n",
        "  - 코사인 유사도를 '두 벡터가 가리키는 방향이 얼마나 비슷한가'를 살펴본다고 생각할 수 있으며, 두 벡터의 방향이 완전히 같으면 1, 완전히 반대면 -1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmp_Ag0dEYny"
      },
      "source": [
        "def cos_similarity(x,y) :\r\n",
        "  nx = x / np.sqrt(np.sum(x**2)) # x의 정규화\r\n",
        "  ny = y / np.sqrt(np.sum(y**2)) # y의 정규화\r\n",
        "  return np.dot(nx, ny)\r\n",
        "\r\n",
        "# 위의 함수는 원소가 모두 0인 벡터가 입력값이 될 경우 'divide by zero' 오류 발생\r\n",
        "# 해결 : 분모에 작은 값을 더함\r\n",
        "# 0.00000001과 같이 작은 값이면 부동소수점 계산 시 '반올림'되어 다른 값에 흡수되어\r\n",
        "# 일반적으로 최종 계산 결과에 영향을 주지 않음\r\n",
        "def cos_similarity(x,y, eps = 1e-8) :\r\n",
        "  nx = x / (np.sqrt(np.sum(x**2)) + eps) # x의 정규화\r\n",
        "  ny = y / (np.sqrt(np.sum(y**2)) + eps) # y의 정규화\r\n",
        "  return np.dot(nx, ny)\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NucSV-NhFpeI",
        "outputId": "d13cfd84-9b4b-44e0-ab95-44812cf639eb"
      },
      "source": [
        "# 'you'와 'i'의 유사도 구하기\r\n",
        "text = 'You say goodbye and I say hello.'\r\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\r\n",
        "vocab_size = len(word_to_id)\r\n",
        "C = create_co_matrix(corpus, vocab_size)\r\n",
        "\r\n",
        "c0 = C[word_to_id['you']] # 'you'의 단어 벡터\r\n",
        "c1 = C[word_to_id['i']] # 'i'의 단어 벡터\r\n",
        "print(cos_similarity(c0, c1)) # 0.7071067691154799 ==> -1에서 1 사이므로, 유사성이 크다고 말할 수 있음"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7071067691154799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obGpZpVsHSZq"
      },
      "source": [
        "### 유사 단어의 랭킹 표시하기\r\n",
        "- 어떤 단어 검색 시 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-rG4J-zGN1W"
      },
      "source": [
        "def most_similarity(query, word_to_id, id_to_word, word_matrix, top=5) :\r\n",
        "  # 검색어 꺼내기\r\n",
        "  if query not in word_to_id :\r\n",
        "    print('%s(을)를 찾을 수 없습니다.' % query)\r\n",
        "    return\r\n",
        "\r\n",
        "  print('\\n[query]' + query) \r\n",
        "  query_id = word_to_id[query]\r\n",
        "  query_vec = word_matrix[query_id] \r\n",
        "\r\n",
        "   # 코사인 유사도 계산\r\n",
        "  vocab_size = len(id_to_word)\r\n",
        "  similarity = np.zeros(vocab_size)\r\n",
        "  \r\n",
        "  for i in range(vocab_size) :\r\n",
        "    similarity[i] = cos_similarity(word_matrix[i], query_vec)\r\n",
        "\r\n",
        "  # 코사인 유사도 기준 내림차순 출력\r\n",
        "  count = 0\r\n",
        "\r\n",
        "  for i in (-1 * similarity).argsort() :\r\n",
        "    if id_to_word[i] == query :\r\n",
        "      continue\r\n",
        "    print('%s:%s' % (id_to_word[i], similarity[i]))\r\n",
        "\r\n",
        "    count += 1\r\n",
        "    \r\n",
        "    if count >= top :\r\n",
        "      return"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIMYlryYIv1z",
        "outputId": "c6bf6847-f373-4200-8696-ccc37e08f985"
      },
      "source": [
        "\r\n",
        "# 'you'로 검색\r\n",
        "most_similarity('you', word_to_id, id_to_word, C, top=5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query]you\n",
            "goodbye:0.7071067691154799\n",
            "i:0.7071067691154799\n",
            "hello:0.7071067691154799\n",
            "say:0.0\n",
            "and:0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0Em8TYEJoxv"
      },
      "source": [
        "## 통계 기반 기법 개선\r\n",
        "- 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타내는데, 단순히 '발생 횟수'를 보는 것은 좋지 않음\r\n",
        "  - 'the'와 'car'은 'car'과 'drive'보다 동시 발생 횟수가 많지만 후자가 더 관련이 깊음\r\n",
        "  - 이와 같이 고빈도 단어가 강한 관련성을 갖는다고 평가될 수 있음\r\n",
        "\r\n",
        "- 이러한 문제를 해결하기 위해 '점별 상호정보량(pointwise mutual information, PMI)라는 척도를 활용함\r\n",
        "  - 단어가 단독으로 출현한 횟수를 고려하여, 고빈도 단어일 경우 PMI 점수가 낮아짐\r\n",
        "\r\n",
        "- PMI의 문제점 : 두 단어의 동시 발생 횟수가 0일 경우, PMI의 값이 음의 무한대가 됨\r\n",
        "\r\n",
        "- 양의 상호정보량(positive PMI, PPMI)  : PMI가 음수일 경우, 0을 취함\r\n",
        "  - 문제점 : 말뭉치의 어휘 수가 증가하면 단어 벡터의 차원수도 증가하고, 대부분이 0인 희소행렬임. 이는 각 원소의 중요도가 낮다는 뜻이고, 이러한 벡터는 노이즈에 약하고 견고하지 못함.\r\n",
        "  - 이러한 문제를 해결하기 위해 자주 수행하는 기법 ==> '벡터의 차원 감소'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4dbINqqI5AC"
      },
      "source": [
        "# C  = 동시발생행렬\r\n",
        "# verbose = 진행상황 출력 여부 플래그\r\n",
        "def ppmi(C, verbose=False, eps=1e-8) :\r\n",
        "  M = np.zeros_like(C, dtype=np.float32)\r\n",
        "  N = np.sum(C) # 전체 단어 발생 횟수\r\n",
        "  S = np.sum(C, axis=0) # 각 단어의 발생 횟수\r\n",
        "  total = C.shape[0] * C.shape[1]\r\n",
        "  cnt = 0\r\n",
        "\r\n",
        "  for i in range(C.shape[0]) :\r\n",
        "    for j in range(C.shape[1]) :\r\n",
        "      pmi = np.log2(C[i,j] * N / (S[j]*S[i]) + eps)\r\n",
        "      M[i,j] = max(0, pmi)\r\n",
        "      \r\n",
        "      if verbose :\r\n",
        "        cnt += 1\r\n",
        "        if cnt % (total/100) == 0:\r\n",
        "          print('%.1f%% 완료' % (100*cnt/total))\r\n",
        "  return M"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdvOom5uM0RS",
        "outputId": "37306491-f01e-40f1-bb40-0025ac0246ec"
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\r\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\r\n",
        "vocab_size = len(word_to_id)\r\n",
        "C = create_co_matrix(corpus, vocab_size)\r\n",
        "W = ppmi(C)\r\n",
        "\r\n",
        "np.set_printoptions(precision=3) # 유효 자릿수를 세자리로 표시\r\n",
        "print('동시발생 행렬')\r\n",
        "print(C)\r\n",
        "print('-'*50)\r\n",
        "print('PPMI')\r\n",
        "print(W)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "동시발생 행렬\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n",
            "--------------------------------------------------\n",
            "PPMI\n",
            "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.807 0.    0.    0.    0.    2.807]\n",
            " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIFwcUGYNvh8"
      },
      "source": [
        "### 차원 감소(dimensionality reduction) \r\n",
        "- 벡터의 차원을 줄이는 되 '중요한 정보'를 최대한 유지하면서 줄이는 기법\r\n",
        "- 데이터의 분포를 고려해 중요한 '축'을 찾는 일을 수행함(데이터를 넓게 분포시키는 축)\r\n",
        "  - 각 데이터의 값은 새로운 축으로 사영된 값으로 변환되며 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 함\r\n",
        "\r\n",
        "- 희소행렬(sparse matrix), 희소벡터(sparse vector) : 원소 대부분이 0인 행렬 또는 벡터를 의미.\r\n",
        "\r\n",
        "- 차원 감소의 핵심 \r\n",
        "  - 희소벡터에서 중요한 축을 찾아 더 적은 차원으로 다시 표현하여, 그 결과로 희소 벡터는 원소 대부분이 0이 아닌 값으로 구성된 밀집벡터로 변환됨. 이 조밀한 벡터가 우리가 원하던 단어의 분산 표현임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAkp-I3GOzmk"
      },
      "source": [
        "#### 차원 감소 방법\r\n",
        "- 특잇값분해(Singular Value Decomposition, SVD)\r\n",
        "  - 임의의 행렬을 세 행렬의 곱으로 분해\r\n",
        "  - 행렬의 크기가 N이면 SVD 계산은 O(N**3)이 걸림. 따라서 Truncated SVD와 같이 특잇값이 작은 값은 버리는 방식으로 성능 향상을 함\r\n",
        "\r\n",
        "- 단어의 벡터 공간에서는 의미가 가아운 단어는 그 거리도 가까울 것으로 기대"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "y-tPzKTWNIzY",
        "outputId": "9ed48969-4db0-401e-e3ea-95afe3168a45"
      },
      "source": [
        "# SVD는 넘파이의 linalg(선형대수의 약어) 모듈이 제공하는 svd 메소드로 실행 가능\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "text = 'You say goodbye and I say hello.'\r\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\r\n",
        "vocab_size = len(word_to_id)\r\n",
        "C = create_co_matrix(corpus, vocab_size)\r\n",
        "W = ppmi(C)\r\n",
        "\r\n",
        "# SVD\r\n",
        "U, S, V = np.linalg.svd(W) # 변수 U에 SVD에 의해 변환된 밀집벡터 표현이 저장됨\r\n",
        "\r\n",
        "# 단어 id가 0인 단어 벡터\r\n",
        "print(C[0]) # 동시발생 행렬\r\n",
        "print(W[0]) # PPMI 행렬\r\n",
        "print(U[0]) # SVD\r\n",
        "\r\n",
        "# 이렇게 변환된 밀집벡터의 차원을 감소시키려면 n개의 원소를 꺼내면 됨\r\n",
        "# 예 ) 2차원 벡터로 차원 축소\r\n",
        "print(U[0, :2])\r\n",
        "\r\n",
        "for word, word_id in word_to_id.items() :\r\n",
        "  plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\r\n",
        "\r\n",
        "plt.scatter(U[:,0], U[:,1], alpha=0.5)\r\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 0]\n",
            "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            "[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
            " -2.426e-17]\n",
            "[0.341 0.   ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoHrplrmTuQj"
      },
      "source": [
        "### PTB 데이터셋\r\n",
        "- 펜 트리뱅그(Penn Trebank)\r\n",
        "\r\n",
        "- word2vec 발명자인 토마스 마콜로프의 웹페이지에서 받은 PTB 말뭉치 사용\r\n",
        "  - PTB 말뭉치에서는 한 문장이 하나의 줄로 저장되어 있고, 각 문장을 연결한 '하나의 큰 시계열 데이터'로 취급\r\n",
        "  - 희소한 단어는 <unk>라는 특수 문자로 치환, 구체적인 숫자는 N으로 대체되어 있음\r\n",
        "  - 각 문장 긑에 <eos>라는 특수 문자 삽입"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5bm5k7ESL7V"
      },
      "source": [
        "# 데이터 셋은 생략하고 코드만 따라쳤음\r\n",
        "# 큰 행렬에 SVD를 적용해야 하므로 고속 SVD를 이용해야 함\r\n",
        "# 그러기 위해서는 sklearn 모듈을 설치해야 함\r\n",
        "\r\n",
        "# try :\r\n",
        "#   # trncated SVD\r\n",
        "#   from sklearn.utils.extmath import randomized_svd\r\n",
        "#   U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\r\n",
        "\r\n",
        "# except ImportError:\r\n",
        "#   # SVD(느리고 메모리 소모가 큼)\r\n",
        "#   U, S, V = np.linalg.svd(W)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4LvlSAaVJn-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}