{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "게이트가 추가된 RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbATpl2jES1W9rZhaA+fj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeaeunJi/deep_learning-/blob/main/%EA%B2%8C%EC%9D%B4%ED%8A%B8%EA%B0%80_%EC%B6%94%EA%B0%80%EB%90%9C_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sLBMRlz9yb1"
      },
      "source": [
        "- 이전에 살펴본 RNN은 순환 경로를 통해 과거의 정보를 기억하는 구조로 단순하여 구현도 쉽게 할 수 있으나 장기 의존 관계(long term, 시간적으로 멀리 떨어진)를 잘 학습하지 못하여 성능이 좋지 못함\r\n",
        "\r\n",
        "- 요즘에는 단순한 RNN보다는 LSTM이나 GRU 계층을 주로 사용하며, RNN이 LSTM을 가리키는 경우도 많음\r\n",
        "\r\n",
        "- 이 LSTM이나 GRU에는 '게이터(gate)'라는 구조가 더해져 있으므로 이 게이트로 시계열 데이터의 장기 의존 관계를 학습할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6ei2I0-kRj"
      },
      "source": [
        "## RNN의 문제점\r\n",
        "- 기본적인 RNN을 통해서는 시계열 데이터의 장기 의존 관계를 학습하기 어려움\r\n",
        "  - BPTT에서 기울기 소실 혹은 기울기 폭발이 일어나기 때문에 학습이 제대로 이루어지지 않음\r\n",
        "\r\n",
        "- 기울기 소실 : 역전파의 기울기 값이 점점 작아지다가 사라지는 현상\r\n",
        "- 기울기 폭발 :기올기 소실과 반대로 매우 큰 수가 되는 현상\r\n",
        "\r\n",
        "### 기울기 소실 또는 기울기 폭발\r\n",
        "- 언어 모델 : 주어진 단어들을 기초로 다음에 출현할 단어를 예측\r\n",
        "\r\n",
        "### 기울기 소실과 기울기 폭발의 원인\r\n",
        "- RNN 계층의 역전파로 전해진 기울기는 차례대로 'tanh', '+', 'MatMul(행렬곱)' 연산을 통과함\r\n",
        "  - + 연산은 상류에서 전해지는 기울기를 그대로 흘려보내므로 기울기는 변하지 않음\r\n",
        "  - tanh(x) 연산은 상류에서 전해지는 기울기는 1-y**2로 이 함수의 미분값은 1.0이하로 x가 0으로부터 멀어질수록 작아짐. 즉, 역전파에서 기울기가 tanh 노드를 지날때마다 값은 계속해서 작아진다는 의미\r\n",
        "  - RNN 계층의 활성화 함수는 주로 tanh 함수를 사용하는데 이를 RuLU로 바꾸면 기울기 소실을 줄일 수 있음(ReLU 함수는 x를 입력하면 max(0,x)를 출력하므로 x가 0이상이면 역전파 시 상류의 기울기가 그대로 하류에 보내지므로 기울기가 작아지지 않음)\r\n",
        "  - 행렬곱 연산은 상류로부터 흘러들어온 기울기 dh*Wh.T로 기울기를 계산하는데, 이때 Wh는 같은 가중치를 사용함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "6crakpjP-j8d",
        "outputId": "546b9b55-2431-4046-bea9-90e4d015fb07"
      },
      "source": [
        "# 역전파 시 기울기는 MatMul 노드를 지날 때마다 어떻게 변하게 될까?\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "N = 2 # 미니배치 크기\r\n",
        "H = 3 # 은닉 상태 벡터의 차원수\r\n",
        "T = 20 # 시계열 데이터의 길이\r\n",
        "\r\n",
        "dh = np.ones((N,H)) # 모든 원소가 1인 행렬\r\n",
        "np.random.seed(3) # 재현 가능하도록 난수의 시드 고정\r\n",
        "Wh = np.random.randn(H,H)\r\n",
        "\r\n",
        "norm_list = []\r\n",
        "for t in range(T) :\r\n",
        "  dh = np.matmul(dh, Wh.T)\r\n",
        "  norm = np.sqrt(np.sum(dh**2)) / N # 미니배치의 평균 L2 노름(각 원소를 제곱하여 모두 더하고 제곱근을 취한 값)\r\n",
        "  norm_list.append(norm)\r\n",
        "\r\n",
        "print(norm_list)\r\n",
        "\r\n",
        "# 그래프 그리기\r\n",
        "plt.plot(np.arange(len(norm_list)), norm_list)\r\n",
        "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\r\n",
        "plt.xlabel('시간 크기(time step)')\r\n",
        "plt.ylabel('노름(norm)')\r\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.4684068094579303, 3.335704974161037, 4.783279375373183, 6.2795873320876145, 8.080776465019055, 10.25116303229294, 12.9360635066099, 16.276861327786712, 20.454829618345983, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.824407307019094, 63.79612654485427, 80.07737014308985, 100.51298922051251, 126.16331847536827, 158.3592064825883, 198.77107967611957, 249.495615421267]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YGP3FXmG8Gj"
      },
      "source": [
        "- 위의 그래프를 보면 기울기의 크기는 시간에 비례하여 지수적으로 증가함\r\n",
        "  - 기울기 폭발(exploding gradients)\r\n",
        "  - 기울기 폭발이 일어나면 결국 오버플로를 일으키고, NaN(Not a Number)같은 값을 발생시킴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "iBMdxOP5FtBQ",
        "outputId": "d493d49f-1d60-4427-9c59-32ef43c44ee8"
      },
      "source": [
        "# 역전파 시 기울기는 MatMul 노드를 지날 때마다 어떻게 변하게 될까?\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "N = 2 # 미니배치 크기\r\n",
        "H = 3 # 은닉 상태 벡터의 차원수\r\n",
        "T = 20 # 시계열 데이터의 길이\r\n",
        "\r\n",
        "dh = np.ones((N,H)) # 모든 원소가 1인 행렬\r\n",
        "np.random.seed(3) # 재현 가능하도록 난수의 시드 고정\r\n",
        "# Wh = np.random.randn(H,H)\r\n",
        "# Wh의 초깃값을 다음과 같이 변경 후 다음 실행 진행\r\n",
        "Wh = np.random.randn(H,H) * 0.5\r\n",
        "\r\n",
        "norm_list = []\r\n",
        "for t in range(T) :\r\n",
        "  dh = np.matmul(dh, Wh.T)\r\n",
        "  norm = np.sqrt(np.sum(dh**2)) / N # 미니배치의 평균 L2 노름(각 원소를 제곱하여 모두 더하고 제곱근을 취한 값)\r\n",
        "  norm_list.append(norm)\r\n",
        "\r\n",
        "print(norm_list)\r\n",
        "\r\n",
        "# 그래프 그리기\r\n",
        "plt.plot(np.arange(len(norm_list)), norm_list)\r\n",
        "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\r\n",
        "plt.xlabel('시간 크기(time step)')\r\n",
        "plt.ylabel('노름(norm)')\r\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.2342034047289652, 0.8339262435402592, 0.5979099219216478, 0.3924742082554759, 0.25252426453184545, 0.16017442237957719, 0.10106299614538984, 0.06358148956166684, 0.039950839098332, 0.025086887541098325, 0.015748611904532892, 0.009884999125204758, 0.006204151282595104, 0.003893806551809953, 0.002443767399386287, 0.0015337065005571367, 0.0009625497320203268, 0.0006040924319556743, 0.00037912574706291117, 0.00023793756048323344]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eg5cm6WHhVE"
      },
      "source": [
        "- 위의 그래프에서는 기울기 dh가 시간 크기에 비례하여 지수적으로 감소\r\n",
        "  - 기울기 소실(vanishing gradients)\r\n",
        "  - 기울기 소실 발생 시 기울기가 매우 빠르게 작아지게 되고, 기울기가 일정 수준 이하로 작아지면 가중치 매개변수가 더이상 갱신되지 않게 됨(장기 의존 관계를 학습할 수 없게 됨)\r\n",
        "\r\n",
        "- 이러한 기울기 폭발과 기울기 소실이 일어나는 이유는 Wh를 T번 반복해서 곱하기 때문에 일어남\r\n",
        "  - 만약 Wh가 스칼라라면 1보다 크면 지수적으로 증가하고, 1보다 작으면 지수적으로 감소\r\n",
        "  - 만약 Wh가 행렬이라면 행렬의 특잇값이 척도가되어 특잇값 중 최댓값이 1보다 크면 지수적으로 증가하고 작으면 지수적으로 감소(반드시 1보다 크다고 일어나는 것을 아님)\r\n",
        "\r\n",
        "  - 행렬의 특잇값 : 데이터가 얼마나 퍼져 있는지를 나타냄 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhxgvZwMIbAV"
      },
      "source": [
        "### 기울기 폭발 대책\r\n",
        "- 전통적인 기법으로 기울기 클리핑(gradients clipping)이라는 기법이 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OIpiAeaHdlq"
      },
      "source": [
        "# 기울기 클리핑 구현\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "dW1 = np.random.rand(3,3) *  10\r\n",
        "dW2 = np.random.rand(3,3) * 10\r\n",
        "grads = [dW1, dW2] # 모든 매개변수의 기울기를 한곳에 모음\r\n",
        "max_norm = 5.0 # Threashold\r\n",
        "\r\n",
        "def clip_grads(grads, max_norm) :\r\n",
        "  total_norm = 0 \r\n",
        "\r\n",
        "  # 기울기에 대한 L2 노름\r\n",
        "  for grad in grads :\r\n",
        "    total_norm += np.sum(grad ** 2)\r\n",
        "  total_norm = np.sqrt(total_norm)\r\n",
        "\r\n",
        "  rate = max_norm / (total_norm + 1e-6)\r\n",
        "  if rate < 1 :\r\n",
        "    for grad in grads :\r\n",
        "      grad *= rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGsbs_enKleT"
      },
      "source": [
        "## 기울기 소실과 LSTM\r\n",
        "- RNN 학습에서 기울기 소실은 큰 문제인데 이를 해결하기 위해서는 RNN 계층의 아키텍쳐 수정이 필요함\r\n",
        "  - 이때, 게이트가 추가된 RNN이 등장\r\n",
        "  - 게이트가 추가된 RNN의 대표로는 LSTM과 GRU가 있음\r\n",
        "\r\n",
        "- LSTM(Long Short-Term Memory) : 단기 기억(short-term memory)를 긴(long) 시간 지속할 수 있음을 의미 \r\n",
        "\r\n",
        "### LSTM의 인터페이스\r\n",
        "- RNN과는 다르게 기억셀(memory cell)이 존재하는데 다른 계층이나 외부로 출력되지 않고 LSTM 계층 내에서만 사용됨\r\n",
        "  - 기억 셀은 시각 t에서의  LSTM의 기억이 저장되며, 과거 ~ 시각 t까지 필요한 모든 정보를 저장\r\n",
        "  - ht는 ct인 기억셀의 값을 tanh()로 변환한 값이며 ct의 각 요소에 tanh()를 적용한 것이 ht이므로 ht와 ct의 원소수는 동일함\r\n",
        "\r\n",
        "- 게이트(gate) : 데이터의 흐름일 제어할 수 있는 문과 같은 역할\r\n",
        "  - 0.0(닫은 상태) ~ 1.0(완전 개발)의 값을 취함\r\n",
        "  - 시그모이드 함수를 사용하여 게이트의 열림 상태를 구함\r\n",
        "  - 열림 상태(openness)를 얼마나 주느냐에 따라 흐리게 할 데이터의 양도 조절 가능\r\n",
        "  - 게이트의 열림 상태는 학습 데이터로부터 자동 학습하여 갱신되며, 게이트의 열림 상태를 제어하기 위한 전용 가중치 매개변수 사용\r\n",
        "\r\n",
        "### output 게이트\r\n",
        "- tanh(ct)에 게이트를 적용한다면?\r\n",
        "  - tanh(ct)의 각 원소에 대해 그것이 다음 시각의 은닉 상태에 얼마나 중요한가를 조정하는 역할\r\n",
        "\r\n",
        "- 다음 은닉 상태 ht의 출력을 담당하는 게이트로 output(출력) 게이트라고 함\r\n",
        "- output 게이트의 열림 상태는 다음 몇 %만 흘려보낼지에 대한 것으로 입력 xt와 이전 상태 h(t-1)의 값으로 구함\r\n",
        "  - 아마다르 곱(Hadamard Product) : 원소별 곱\r\n",
        "  -  sigmoid(xt*wx + h(t-1)*wh + b)\r\n",
        "\r\n",
        "- tanh의 출력은 -1.0 ~ 1.0의 실수를 출력하고, 그 수치는 인코딩된 정보의 강약(정도)를 표시한다고 해석할 수 있음\r\n",
        "- sigmoid의 출력은 0.0 ~ 1.0이며 데이터를 얼마만큼 통과시킬지를 정하는 비율을 의미\r\n",
        "\r\n",
        "### forget 게이트\r\n",
        "- 기얼셀에서 불필요한 기억을 잊도록 하는 망각 게이트\r\n",
        "- sigmoid(xt * wx + h(t-1)*Wh + b)\r\n",
        "- ct = f와 c(t-1)의 원소별 곱\r\n",
        "\r\n",
        "### 새로운 기억 셀\r\n",
        "- 새로 기억해야 할 정보를 기억하는 셀\r\n",
        "- 이전 시각의 기억 셀에 tanh 노드가 계산할 경과가 더해짐\r\n",
        "  - 기억 셀에 새로운 정보가 추가된 것임\r\n",
        "  - tanh 노드는 게이트가 아니고, 새로운 정보를 추가하는 것이 목적임\r\n",
        "\r\n",
        "### input 게이트\r\n",
        "- 기억 셀에 추가된 새로운 정보의 가치가 얼마나 큰지를 판단하는 게이트\r\n",
        "- input 게이트에 의해 가중치 정보가 새로 추가되는 것과 비슷\r\n",
        "- 새로 추가될 정보와 input로 계산된 값은 원소별 곱하고 이를 기억 셀에 추가\r\n",
        "\r\n",
        "### LSTM의 기울기 흐름\r\n",
        "- 기억 셀에만 집중하여 역전파의 흐름을 살펴보면 기억 셀의 역전파에서는 +와 * 만 지나게 되는데, + 노드는 상류에서 전해주는 기울기를 그대로 하류로 전달하므로 기울기 변화가 없음\r\n",
        "- * 연산은 원소별 곱으로 계산되고 매 시각 f라는 다른 게이트 값을 이용하여 원소별 곱을 계산하기 때문에 곱셈의 효과가 누적되지 않아 기울기 소실이 일어나기 어려움\r\n",
        "- * 연산은 forget 게이트가 제어하고 매 시각 다른 게이트 값을 출력\r\n",
        "  - 해당 게이트가 잊어야 한다고 판단한 기억 셀의 원소에 대해서는 기울기가 작아지지만 오래 기억해야 할 정보즤 경우에는 기울기가 약해지지 않을 채 하류로 전해짐\r\n",
        "  - 따라서 기억 셀의 기울기가 오래 기억해야할 정보일 경우 소실 없이 전파되리라 기대 가능\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTiEROU4WWwk"
      },
      "source": [
        "class LSTM :\r\n",
        "  def __init__(self, Wx, Wh, b) :\r\n",
        "    self.params = [Wx, Wh, b] # 해당 매개변수에는 4개의 가주치와 편향이 담겨 있음\r\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\r\n",
        "    self.cache = None # 순전파 때 중간 결과를 보관 후 역전파 계산에 사용하는 인스턴스 변수\r\n",
        "\r\n",
        "  def forward(self, x, h_prev, c_prev) :\r\n",
        "    Wx, Wh, b = self.params\r\n",
        "    N, H = h_perv.shape\r\n",
        "\r\n",
        "    A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b\r\n",
        "\r\n",
        "    # slice\r\n",
        "    f = A[:, :H]\r\n",
        "    g = A[:, H:2*H]\r\n",
        "    i = A[:, 2*H:3*H]\r\n",
        "    o = A[:, 3*H:]\r\n",
        "\r\n",
        "    f = sigmoid(f)\r\n",
        "    g = np.tanh(g)\r\n",
        "    i = sigmoid(i)\r\n",
        "    o = sigmoid(o)\r\n",
        "\r\n",
        "    c_next = f * c_prev + g * i\r\n",
        "    h_next = o * np.tanh(c_next)\r\n",
        "\r\n",
        "    self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\r\n",
        "    return h_next, c_next\r\n",
        "\r\n",
        "  def backward(self, dh_next, dc_next):\r\n",
        "    Wx, Wh, b = self.params\r\n",
        "    x, h_prev, c_prev, i, f, g, o, c_next = self.cache\r\n",
        "\r\n",
        "    tanh_c_next = np.tanh(c_next)\r\n",
        "\r\n",
        "    ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\r\n",
        "\r\n",
        "    dc_prev = ds * f\r\n",
        "\r\n",
        "    di = ds * g\r\n",
        "    df = ds * c_prev\r\n",
        "    do = dh_next * tanh_c_next\r\n",
        "    dg = ds * i\r\n",
        "\r\n",
        "    di *= i * (1 - i)\r\n",
        "    df *= f * (1 - f)\r\n",
        "    do *= o * (1 - o)\r\n",
        "    dg *= (1 - g ** 2)\r\n",
        "\r\n",
        "    dA = np.hstack((df, dg, di, do))\r\n",
        "\r\n",
        "    dWh = np.dot(h_prev.T, dA)\r\n",
        "    dWx = np.dot(x.T, dA)\r\n",
        "    db = dA.sum(axis=0)\r\n",
        "\r\n",
        "    self.grads[0][...] = dWx\r\n",
        "    self.grads[1][...] = dWh\r\n",
        "    self.grads[2][...] = db\r\n",
        "\r\n",
        "    dx = np.dot(dA, Wx.T)\r\n",
        "    dh_prev = np.dot(dA, Wh.T)\r\n",
        "\r\n",
        "    return dx, dh_prev, dc_prev"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKc71auzaFrA"
      },
      "source": [
        "### Time LSTM 구현\r\n",
        "# T개분의 시계열 데이터를 한꺼번에 처리하는 계층\r\n",
        "# Truncated BPTT는 역전파의 연결을 적당한 블록으로 끊어서 하지만 순전파의 흐름을 그대로 \r\n",
        "# 유지해야 하므로 은닉 상태와 기억 셀을 인스턴스 변수로 유지\r\n",
        "class TImeLSTM:\r\n",
        "  def __init__(self, Wx, Wh, b, stateful=False) :\r\n",
        "    self.parmas [Wx, Wh, b]\r\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\r\n",
        "    self.layers = None\r\n",
        "    self.h, self.c = None, None \r\n",
        "    self.dh = None\r\n",
        "    self.stateful = stateful\r\n",
        "\r\n",
        "  def forward(self, xs) :\r\n",
        "    Wx, Wh, b = self.parmas\r\n",
        "    N, T, D = xs.shape\r\n",
        "    H = Wh.shape[0]\r\n",
        "\r\n",
        "    self.layers = {}\r\n",
        "    hs = np.empty((N,T,H), dtype='f')\r\n",
        "\r\n",
        "    if not self.stateful or self.h is None :\r\n",
        "      self.h = np.zeros((N,H), dtype='f')\r\n",
        "    if not self.stateful or self.c is None :\r\n",
        "      self.c = np.zeros((N,H), dtype='f')\r\n",
        "\r\n",
        "    for t in ragne(T):\r\n",
        "      layer = LSTM(*self.params)\r\n",
        "      self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\r\n",
        "      hs[:, t, :] = self.h\r\n",
        "\r\n",
        "      self.layers.append(layer)\r\n",
        "    \r\n",
        "    return hs\r\n",
        "\r\n",
        "  def backward(self, dhs) :\r\n",
        "    Wx, wh, b= self.params\r\n",
        "    N, T, H = dhs.shape\r\n",
        "    D = Wx.shape[0]\r\n",
        "\r\n",
        "    dxs = np.empty((N, T, D), dtype='f')\r\n",
        "    dh, dc = 0,0\r\n",
        "\r\n",
        "    grads = [0,0,0]\r\n",
        "\r\n",
        "    for t in reversed(range(T)) :\r\n",
        "      layer = self.layers[t]\r\n",
        "      dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\r\n",
        "      dxs[:, t, :] = dx\r\n",
        "      for i, grad in enumerate(layer.grads) :\r\n",
        "        grads[i] += grad\r\n",
        "\r\n",
        "    for i, grad in enumerate(grads) :\r\n",
        "      self.grads[i][...] = grad\r\n",
        "    self.dh = dh\r\n",
        "    return dxs\r\n",
        "\r\n",
        "  def set_state(self, h, c=None) :\r\n",
        "    self.h, self.c = h,c\r\n",
        "  \r\n",
        "  def reset_state(self) :\r\n",
        "    self.h, self.c = None, None\r\n",
        "    \r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKq6-NMocu3l"
      },
      "source": [
        "## LSTM을 사용한 언어 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgA6m_1EcsAX"
      },
      "source": [
        " import pickle\r\n",
        "\r\n",
        " class Rnnlm :\r\n",
        "   def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100) :\r\n",
        "     V, D, H = vovab_size, wordvec_size, hidden_size\r\n",
        "     rn = np.random.randn\r\n",
        "\r\n",
        "    # 가중치 초기화\r\n",
        "    embed_W = (rn(V, D) / 100).astype('f') \r\n",
        "    lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\r\n",
        "    lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\r\n",
        "    lstm_b = np.zeros(4 * H).astype('f')\r\n",
        "    affine_W = (rn(H,V) / np.sqrt(H)).astype('f')\r\n",
        "    affine_b = np.zeros(V).astype('f')\r\n",
        "\r\n",
        "    # 계층 생성\r\n",
        "    self.layers = [\r\n",
        "        TimeEmbedding(embed_W),\r\n",
        "        TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\r\n",
        "        TimeAffine(affine_W, affine_b)\r\n",
        "    ]\r\n",
        "\r\n",
        "    self.loss_layer = TimeSoftmaxWithLoss()\r\n",
        "    self.lstm_layer = self.layers[1]\r\n",
        "\r\n",
        "    # 모든 가중치와 기울기를 리스트에 모음\r\n",
        "    self.params, self.grads = [], []\r\n",
        "    for layer in self.layers :\r\n",
        "      self.params += layer.params\r\n",
        "      self.grads += layer.grads\r\n",
        "\r\n",
        "  def predict(self, xs) :\r\n",
        "    for layer in self.layers :\r\n",
        "      xs = layer.forward(xs)\r\n",
        "    return xs\r\n",
        "\r\n",
        "  def forward(self, xs, ts) :\r\n",
        "    score = self.predict(xs)\r\n",
        "    loss = self.loss_layer.forward(score, ts)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def backward(self, dout=1) :\r\n",
        "    dout = self.loss_layer.backward(dout)\r\n",
        "    for layer in reversed(self.layers) :\r\n",
        "      dout = layer.backward(dout)\r\n",
        "    return dout\r\n",
        "\r\n",
        "  def reset_state(self) :\r\n",
        "    self.lstm_layer.reset_state()\r\n",
        "\r\n",
        "  def save_params(self, file_name='Rnnlm.pkl') :\r\n",
        "    with open(file_name, 'wb') as f :\r\n",
        "      pickle.dump(self.params, f)\r\n",
        "  \r\n",
        "  def load_params(self, file_name='Rnnlm.pkl') :\r\n",
        "    with open(file_name, 'rb') as f :\r\n",
        "      self.params = pickle.load(f)\r\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKjUNrY6gR33"
      },
      "source": [
        "# 모델 학습 후 구한 params, grads \r\n",
        "# 기울기 클리핑\r\n",
        "# if max_grad is not None :\r\n",
        "#   clip_grads(grads, max_grad)\r\n",
        "# # 매개변수 갱신\r\n",
        "# optimizer.update(params, grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxC9CwqKf75i"
      },
      "source": [
        "## RNNL 추가 개선\r\n",
        "### LSTM 계층 다층화\r\n",
        "- RNNLM으로 정확한 모델을 만들고자 한다면 많은 경우, LSTM 계층을 깊게 쌓아서 성능을 향상시킬 수 있음\r\n",
        "### 드롭아웃에 의한 과적합 억제\r\n",
        "- 다층화로 시계열 데이터의 복잡한 의존 관계를 학습한 모델에서 과적합 문제가 일어날 수 있음\r\n",
        "- 일반적으로 RNN은 피드포워드 신경망보다 쉽게 과적합을 일으킴\r\n",
        "\r\n",
        "- 과적합 억제 방법\r\n",
        "  - 훈련 데이터의 양 늘리기\r\n",
        "  - 모델의 복잡도를 줄이기\r\n",
        "  - 정규화를 통해 모델의 복잡도에 패널티를 주는 기법(L2 정규화는 가중치가 너무 커지면 패널티를 부과)\r\n",
        "  - 드롭 아웃(훈련 시 계층 내의 뉴런 몇 개를 무작위로 무시하고 학습하는 방법)\r\n",
        "  - 변형 드롭  아웃(상하방향 + 좌우 방향)\r\n",
        "  - 임베딩 계층과 affine 계층 사이의 가중치 공유\r\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmDldqoD-8k_"
      },
      "source": [
        "### 3가지 개선점\r\n",
        "# 1. LSTM 계층의 다층화(2층)\r\n",
        "# 2. 드롭아웃 사용(깊이 방향으로만 적용)\r\n",
        "# 3. 가중치 공유(Embedding 계층과 Affine 계층에서 가중치 공유)\r\n",
        "# 작동되지 않는 코드\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('..')\r\n",
        "from common.time_layers import *\r\n",
        "from common.np import *\r\n",
        "from common.base_model import BaseModel\r\n",
        "\r\n",
        "class BetterRnnlm(BaseModel) :\r\n",
        "  def __init__(self, vocab_size=10000, wordvec_size=650,\r\n",
        "               hidden_size=650, dropout_ratio=0.5) :\r\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\r\n",
        "    rn = np.random.randn\r\n",
        "\r\n",
        "    embed_W = (rn(V,D) / 100).astype('f')\r\n",
        "    lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\r\n",
        "    lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\r\n",
        "    lstm_b1 = np.zeros(4 * H).astype('f')\r\n",
        "    lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\r\n",
        "    lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f') \r\n",
        "    lstm_b2 = np.zeros(4 * H).astype('f')\r\n",
        "    affine_b = np.zeros(V).astype('f')\r\n",
        "\r\n",
        "    # 세가지 개선\r\n",
        "    self.layers = [\r\n",
        "      TimeEmbedding(embed_W),\r\n",
        "      TimeDropout(dropout_ratio),\r\n",
        "      TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\r\n",
        "      TimeDropout(dropout_ratio),\r\n",
        "      TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\r\n",
        "      TimeDropout(dropout_ratio),\r\n",
        "      TimeAffine(embed_W.T, affine_b) # 가중치 공유                \r\n",
        "    ]\r\n",
        "\r\n",
        "    self.loss_layer = TimeSoftmaxWithLoss()\r\n",
        "    self.lstm_layers = [self.layers[2], self.layers[4]]\r\n",
        "    self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\r\n",
        "\r\n",
        "    self.params, self.grads = [], []\r\n",
        "\r\n",
        "    for layer in self.layers :\r\n",
        "      self.params += layer.params\r\n",
        "      self.grads += layer.grads\r\n",
        "    \r\n",
        "  def predict(slef, xs, train_flg=False) :\r\n",
        "    for layer in self.drop_layers :\r\n",
        "      layer.train_flg = train_flg\r\n",
        "    for layer in self.layers :\r\n",
        "      xs = layer.forard(xs)\r\n",
        "    return xs\r\n",
        "\r\n",
        "  def forward(self, xs, ts, train_flg=True) :\r\n",
        "    score = self.predict(xs, train_flg)\r\n",
        "    loss = self.loss_layer.forward(score, ts)\r\n",
        "    return loss\r\n",
        "  \r\n",
        "  def backward(self, dout=1) :\r\n",
        "    dout = self.loss_layer.backward(dout)\r\n",
        "    for layer in reversed(self.layers):\r\n",
        "      dout = layer.backward(dout)\r\n",
        "    return dout\r\n",
        "  \r\n",
        "  def reset_state(self) :\r\n",
        "    for layer in self.lstm_layers :\r\n",
        "      layer.reset_state()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b08ge9WYC_Z7"
      },
      "source": [
        "# 작동되지 않은 코드\r\n",
        "# 매 에폭에서 검증 데이터로 퍼플렉서티를 평가하고, 그 값이 나빠졌을 경우에만 학습률을 낮춤\r\n",
        "import sys\r\n",
        "sys.path.append('..')\r\n",
        "from common import config\r\n",
        "\r\n",
        "from common.optimizer import SGD\r\n",
        "from common.trainer import RnnlmTrainer\r\n",
        "from common.util import eval_perplexity\r\n",
        "from dataset import ptb \r\n",
        "from better_rnnlm import BetterRnnlm\r\n",
        "\r\n",
        "# 하이퍼 파라미터 설정\r\n",
        "batch_size = 20\r\n",
        "wordvec_size = 650\r\n",
        "hidden_size = 650\r\n",
        "time_size = 35\r\n",
        "lr = 20.0\r\n",
        "max_epoch = 40\r\n",
        "max_grad = 0.25\r\n",
        "dropout = 0.5\r\n",
        "\r\n",
        "# 학습 데이터 읽기\r\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\r\n",
        "corpus_val, _, _ = ptb.load_data('val')\r\n",
        "corpus_text, _, _ = ptb.load_data('test')\r\n",
        "\r\n",
        "vocab_size = len(word_to_id)\r\n",
        "xs = corpus[:-1]\r\n",
        "ts = corpus[1:]\r\n",
        "\r\n",
        "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\r\n",
        "optimizer = SGD(lr)\r\n",
        "trainer = RnnlmTrainer(model, optimizer)\r\n",
        "\r\n",
        "best_ppl = float('inf')\r\n",
        "for epoch in range(max_epoch) :\r\n",
        "  trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\r\n",
        "              time_size=time_size, max_grad=max_grad)\r\n",
        "  \r\n",
        "  model.reset_state() # 학습이 끝난 후 테스트 데이터를 사용하여 퍼플렉서티 평가 시 모델 상태를 재설정하여 평가를 수행\r\n",
        "  ppl = eval_perplexity(model, corpus_val)\r\n",
        "\r\n",
        "  if best_ppl > ppl :\r\n",
        "    bset_ppl = ppl\r\n",
        "    model.save_params()\r\n",
        "  else :\r\n",
        "    lr /=4.0 \r\n",
        "    optimizer.lr = lr\r\n",
        "  \r\n",
        "  model.reset_state()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}